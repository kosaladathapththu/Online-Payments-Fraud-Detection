# -*- coding: utf-8 -*-
"""Online_Payments_Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bx37GCFK0lMgCT-tgfTUvV4dbIFtdZoN

Mount Drive + Load dataset
"""

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd

path = "/content/drive/MyDrive/FraudProject_New/Online_Payments_Fraud_Detection.csv"
df = pd.read_csv(path)

print("Shape:", df.shape)
df.head()

"""Quick checks + Class distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

print(df.isnull().sum().sum())
print(df["isFraud"].value_counts())
print(df["isFraud"].value_counts(normalize=True))

plt.figure(figsize=(5,4))
sns.countplot(x="isFraud", data=df)
plt.title("Class Distribution (isFraud)")
plt.show()

"""Split Data"""

!pip install imbalanced-learn

"""Evaluate with better metrics (fraud)"""

from sklearn.metrics import average_precision_score, precision_recall_curve

y_prob = rf.predict_proba(X_test)[:, 1]
pr_auc = average_precision_score(y_test, y_prob)
print("PR-AUC:", pr_auc)

"""Tune the threshold (to improve precision)"""

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

y_prob = rf.predict_proba(X_test)[:, 1]

for t in [0.3, 0.4, 0.5, 0.6, 0.7]:
    y_pred_t = (y_prob >= t).astype(int)
    p = precision_score(y_test, y_pred_t)
    r = recall_score(y_test, y_pred_t)
    f1 = f1_score(y_test, y_pred_t)
    print(t, "precision:", p, "recall:", r, "f1:", f1)

"""cross validation"""

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

X_small, _, y_small, _ = train_test_split(
    X_train, y_train,
    train_size=200000,
    stratify=y_train,
    random_state=42
)

pipe = Pipeline([
    ("smote", SMOTE(sampling_strategy=0.1, random_state=42)),
    ("rf", RandomForestClassifier(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42))
])

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
scores = cross_val_score(pipe, X_small, y_small, cv=cv, scoring="average_precision", n_jobs=-1)
print("CV PR-AUC:", scores, "Mean:", scores.mean())

"""Tune the Decision Threshold"""

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

y_prob = rf.predict_proba(X_test)[:, 1]

for t in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:
    y_pred_t = (y_prob >= t).astype(int)
    print("Threshold:", t)
    print("Precision:", precision_score(y_test, y_pred_t))
    print("Recall:", recall_score(y_test, y_pred_t))
    print("F1:", f1_score(y_test, y_pred_t))
    print("-------------------")

"""Check Feature Importance"""

import pandas as pd

feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)
feature_importance.sort_values(ascending=False).head(10)

"""Split first"""

from sklearn.model_selection import train_test_split

X = df.drop("isFraud", axis=1)
y = df["isFraud"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

"""Preprocess (encoding/scaling) using a Pipeline"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline

cat_cols = X_train.select_dtypes(include=["object"]).columns
num_cols = X_train.select_dtypes(exclude=["object"]).columns

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", "passthrough", num_cols)
    ]
)

"""Setup (install + imports)"""

!pip -q install imbalanced-learn xgboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from xgboost import XGBClassifier

""" Helper: evaluate any model (ROC‑AUC + PR‑AUC)"""

def eval_model(name, model, X_test, y_test):
    proba = model.predict_proba(X_test)[:, 1]
    roc = roc_auc_score(y_test, proba)
    pr  = average_precision_score(y_test, proba)
    print(f"{name} | ROC-AUC: {roc:.6f} | PR-AUC: {pr:.6f}")
    return proba, roc, pr

"""Step 1 — Load data + show original class distribution"""

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

path = "/content/drive/MyDrive/FraudProject_New/Online_Payments_Fraud_Detection.csv"
df = pd.read_csv(path)

print("Full Shape:", df.shape)
print(df["isFraud"].value_counts())
print(df["isFraud"].value_counts(normalize=True))

plt.figure(figsize=(5,4))
sns.countplot(x="isFraud", data=df)
plt.title("Original Class Distribution (Full Data)")
plt.show()

"""Take 200K Sample (Stratified)"""

# Take 200K sample (keep fraud ratio same)
df_sample = df.sample(n=200000, random_state=42)

print("Sample Shape:", df_sample.shape)

# Check class distribution
print(df_sample['isFraud'].value_counts())
print(df_sample['isFraud'].value_counts(normalize=True))

""" Prepare Features"""

df_sample = df_sample.drop(columns=['nameOrig', 'nameDest'])

X = df_sample.drop('isFraud', axis=1)
y = df_sample['isFraud']

"""Train Test Split (VERY IMPORTANT)"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

print("Before SMOTE:")
print(y_train.value_counts())

"""Apply SMOTE (ONLY on Training Data)"""

from imblearn.over_sampling import SMOTE
import pandas as pd

# One-hot encode the 'type' column in X_train and X_test
X_train = pd.get_dummies(X_train, columns=['type'], drop_first=True)
X_test = pd.get_dummies(X_test, columns=['type'], drop_first=True)

smote = SMOTE(random_state=42)

X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("After SMOTE:")
print(y_train_sm.value_counts())

"""Train Random Forest (Faster Version)"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=12,
    n_jobs=-1,
    random_state=42
)

rf.fit(X_train_sm, y_train_sm)

"""Evaluate Model"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_pred = rf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""ROC Curve (Important)"""

from sklearn.metrics import roc_curve, roc_auc_score

y_prob = rf.predict_proba(X_test)[:,1]

fpr, tpr, _ = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc:.4f}")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

print("AUC Score:", auc)

"""Before SMOTE (original training)"""

print("y_train (before SMOTE):")
print(y_train.value_counts())
print(y_train.value_counts(normalize=True))

"""After SMOTE (balanced training)"""

import pandas as pd

print("y_train_sm (after SMOTE):")
print(pd.Series(y_train_sm).value_counts())
print(pd.Series(y_train_sm).value_counts(normalize=True))

"""Test set (must stay unbalanced)"""

print("y_test (unchanged):")
print(y_test.value_counts())
print(y_test.value_counts(normalize=True))

"""Plot"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(5,4))
sns.countplot(x=y_train)
plt.title("Train Before SMOTE")
plt.show()

plt.figure(figsize=(5,4))
sns.countplot(x=y_train_sm)
plt.title("Train After SMOTE")
plt.show()

plt.figure(figsize=(5,4))
sns.countplot(x=y_test)
plt.title("Test (Should Stay Imbalanced)")
plt.show()

"""Balanced Class Distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

print("Class Counts:")
print(y_train.value_counts())

print("\nClass Percentage:")
print(y_train.value_counts(normalize=True))

sns.countplot(x=y_train)
plt.title("Balanced Class Distribution (Training Data)")
plt.show()

from collections import Counter

print("Before SMOTE:", Counter(y_train))

print("After SMOTE:", Counter(y_train_sm))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train_sm, y_train_sm)

y_pred = rf.predict(X_test)

print(classification_report(y_test, y_pred))

print("Train (SMOTE):", y_train_sm.value_counts())
print("Test:", y_test.value_counts())

"""****************************************************************************************

Check ROC Curve & AUC
"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_prob = rf.predict_proba(X_test)[:,1]

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

plt.plot(fpr, tpr)
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(f"ROC Curve (AUC = {auc:.4f})")
plt.show()

print("AUC Score:", auc)

"""Precision‑Recall Curve"""

from sklearn.metrics import precision_recall_curve
import numpy as np

precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

plt.plot(recall, precision)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.show()

"""Threshold Tuning"""

# Example: lower threshold to 0.3
threshold = 0.3
y_pred_new = (y_prob >= threshold).astype(int)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_new))

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_sm, y_train_sm)

y_pred_lr = lr.predict(X_test)
print("Logistic Regression")
print(classification_report(y_test, y_pred_lr))

"""XGBoost (Very Strong for Fraud)"""

from xgboost import XGBClassifier

xgb = XGBClassifier(eval_metric='logloss')
xgb.fit(X_train_sm, y_train_sm)

y_pred_xgb = xgb.predict(X_test)
print("XGBoost")
print(classification_report(y_test, y_pred_xgb))

"""Compare All Models"""

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

models = {
    "RandomForest": rf,
    "LogisticRegression": lr,
    "XGBoost": xgb
}

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"\n{name}")
    print("Recall:", recall_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("F1:", f1_score(y_test, y_pred))

"""import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(x=y_train)
plt.title("Class Distribution - Before SMOTE")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(x=y_train)
plt.title("Class Distribution - Before SMOTE")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

"""After SMOTE (Balanced)"""

import pandas as pd

plt.figure(figsize=(6,4))
sns.countplot(x=pd.Series(y_train_sm))
plt.title("Class Distribution - After SMOTE (Balanced)")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

"""Boxplots (Feature Distribution)"""

plt.figure(figsize=(14,6))
sns.boxplot(data=X_train_sm)
plt.xticks(rotation=90)
plt.title("Boxplot of Features (Balanced Data)")
plt.show()

"""Correlation Heatmap"""

plt.figure(figsize=(12,10))
sns.heatmap(pd.DataFrame(X_train_sm).corr(),
            cmap="coolwarm",
            annot=False)
plt.title("Correlation Heatmap - Balanced Data")
plt.show()

"""Feature Importance (Random Forest)"""

import pandas as pd

feature_imp = pd.DataFrame({
    'Feature': X_train_sm.columns,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8,6))
sns.barplot(x='Importance', y='Feature', data=feature_imp)
plt.title("Feature Importance - Random Forest")
plt.show()

"""Confusion Matrix (Diagram)"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, rf.predict(X_test))

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""ROC Curve"""

from sklearn.metrics import roc_curve, auc

y_prob = rf.predict_proba(X_test)[:,1]

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=500)
lr.fit(X_train, y_train)

print("Logistic Regression Accuracy:",
      lr.score(X_test, y_test))

"""Cross Validation"""

from sklearn.ensemble import RandomForestClassifier

rf_reg = RandomForestClassifier(
    n_estimators=200,
    max_depth=12,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

rf_reg.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score
import pandas as pd

# One-hot encode the 'type' column in X
X_encoded = pd.get_dummies(X, columns=['type'], drop_first=True)

cv_scores = cross_val_score(rf_reg, X_encoded, y, cv=5, n_jobs=-1)

print("Cross Validation Scores:", cv_scores)
print("Mean CV Score confined to 5 fold evaluation of cross_val_score and RandomForestClassifier:", cv_scores.mean())

"""Compare Multiple Models"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(name, "Accuracy:", accuracy_score(y_test, y_pred))

"""Create Model Comparison Chart"""

import matplotlib.pyplot as plt

model_names = []
accuracies = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    model_names.append(name)
    accuracies.append(accuracy_score(y_test, y_pred))

plt.bar(model_names, accuracies)
plt.title("Model Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=30)
plt.show()

"""Threshold Tuning (For Fraud Detection)"""

y_probs = rf_reg.predict_proba(X_test)[:,1]

custom_threshold = 0.3
y_custom = (y_probs > custom_threshold).astype(int)

print("Train (before SMOTE):\n", y_train.value_counts())
print("Test (must stay imbalanced):\n", y_test.value_counts())
print("Train (after SMOTE):\n", pd.Series(y_train_sm).value_counts())

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score

model = rf_reg   # your best model

y_probs = model.predict_proba(X_test)[:, 1]
y_pred  = model.predict(X_test)

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, y_probs))
print("PR-AUC :", average_precision_score(y_test, y_probs))

"""Tune Threshold"""

for t in [0.5, 0.4, 0.3, 0.2, 0.1]:
    y_custom = (y_probs >= t).astype(int)
    print("\n--- Threshold:", t, "---")
    print(classification_report(y_test, y_custom, digits=4))

"""Plot ROC Curve"""

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_probs)

plt.plot(fpr, tpr)
plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.show()

"""Plot Precision‑Recall Curve"""

from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, y_probs)

plt.plot(recall, precision)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.show()

import joblib
joblib.dump(rf_reg, "final_fraud_model.pkl")

"""TESTing Accuracy"""

from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
from google.colab import drive

# 1. Load data and create df
drive.mount("/content/drive", force_remount=True) # Ensure drive is mounted
path = "/content/drive/MyDrive/FraudProject_New/Online_Payments_Fraud_Detection.csv"
df = pd.read_csv(path)

# 2. Take 200K sample
df_sample = df.sample(n=200000, random_state=42)

# 3. Prepare Features
df_sample = df_sample.drop(columns=['nameOrig', 'nameDest'])
X = df_sample.drop('isFraud', axis=1)
y = df_sample['isFraud']

# Re-create X_train, X_test, y_train, y_test for robustness
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# One-hot encode the 'type' column in X_train and X_test, similar to previous preprocessing
X_train = pd.get_dummies(X_train, columns=['type'], drop_first=True)
X_test = pd.get_dummies(X_test, columns=['type'], drop_first=True)

# Re-define and fit rf_reg to ensure it's available
rf_reg = RandomForestClassifier(
    n_estimators=200,
    max_depth=12,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)
rf_reg.fit(X_train, y_train)

y_pred_test = rf_reg.predict(X_test)

test_accuracy = accuracy_score(y_test, y_pred_test)

print("Testing Accuracy:", test_accuracy)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_test, digits=4))

from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
from google.colab import drive

# Ensure drive is mounted and data is loaded, split, and processed
drive.mount("/content/drive", force_remount=True)
path = "/content/drive/MyDrive/FraudProject_New/Online_Payments_Fraud_Detection.csv"
df = pd.read_csv(path)
df_sample = df.sample(n=200000, random_state=42)
df_sample = df_sample.drop(columns=['nameOrig', 'nameDest'])
X = df_sample.drop('isFraud', axis=1)
y = df_sample['isFraud']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

X_train = pd.get_dummies(X_train, columns=['type'], drop_first=True)
X_test = pd.get_dummies(X_test, columns=['type'], drop_first=True)

# Redefine and fit rf_reg to ensure it's available and trained
rf_reg = RandomForestClassifier(
    n_estimators=200,
    max_depth=12,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)
rf_reg.fit(X_train, y_train)

y_probs = rf_reg.predict_proba(X_test)[:,1]

for t in [0.5, 0.4, 0.3, 0.2]:
    y_custom = (y_probs >= t).astype(int)
    print("\nThreshold:", t)
    print(classification_report(y_test, y_custom, digits=4))

"""Save directly to Google Drive"""

from google.colab import drive
drive.mount("/content/drive")

import joblib
import os

save_path = "/content/drive/MyDrive/FraudProject_New/models/fraud_rf_model.pkl"

# Create the directory if it doesn't exist
os.makedirs(os.path.dirname(save_path), exist_ok=True)

joblib.dump(rf_reg, save_path)
print("Saved to:", save_path)

"""Save to Colab local"""

import joblib

joblib.dump(rf_reg, "fraud_rf_model.pkl")
print("Saved: fraud_rf_model.pkl")

"""Learning Curve Code"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from imblearn.over_sampling import SMOTE

# Use your final trained model
model = rf_reg

# Re-create SMOTE data if not available
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

train_sizes = np.linspace(0.1, 1.0, 10)

train_sizes, train_scores, test_scores = learning_curve(
    model,
    X_train_sm,          # use SMOTE training data
    y_train_sm,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

# Calculate mean and std
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot
plt.figure(figsize=(8,6))
plt.plot(train_sizes, train_mean, 'o-', label="Training Accuracy")
plt.plot(train_sizes, test_mean, 'o-', label="Validation Accuracy")

plt.fill_between(train_sizes,
                 train_mean - train_std,
                 train_mean + train_std,
                 alpha=0.2)

plt.fill_between(train_sizes,
                 test_mean - test_std,
                 test_mean + test_std,
                 alpha=0.2)

plt.xlabel("Training Set Size")
plt.ylabel("Accuracy")
plt.title("Learning Curve - Random Forest")
plt.legend()
plt.grid(True)
plt.show()

""" Model evaluation"""

from sklearn.metrics import (confusion_matrix, classification_report,
                             roc_auc_score, average_precision_score)

# predictions
y_pred  = rf_reg.predict(X_test)
y_proba = rf_reg.predict_proba(X_test)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))

print("ROC-AUC:", roc_auc_score(y_test, y_proba))
print("PR-AUC :", average_precision_score(y_test, y_proba))  # VERY IMPORTANT for fraud

# 1. Split first
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# One-hot encode the 'type' column after splitting and before SMOTE
X_train = pd.get_dummies(X_train, columns=['type'], drop_first=True)
X_test = pd.get_dummies(X_test, columns=['type'], drop_first=True)

# 2. Apply SMOTE ONLY on training data
sm = SMOTE()
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# 3. Train model
model.fit(X_train_res, y_train_res)

# 4. Evaluate on X_test (NOT resampled test)
model.score(X_test, y_test)

"""Threshold tuning"""

from sklearn.metrics import precision_score, recall_score, f1_score

for t in [0.5, 0.4, 0.3, 0.2, 0.1]:
    pred_t = (y_proba >= t).astype(int)
    p = precision_score(y_test, pred_t)
    r = recall_score(y_test, pred_t)
    f1 = f1_score(y_test, pred_t)
    print(f"t={t:.1f}  precision={p:.4f}  recall={r:.4f}  f1={f1:.4f}")

"""Hyperparameter tuning (optimize Random Forest)"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42, n_jobs=-1)

param_dist = {
    "n_estimators": [100, 200, 300],
    "max_depth": [8, 12, 16, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["sqrt", "log2", None],
}

search = RandomizedSearchCV(
    rf,
    param_distributions=param_dist,
    n_iter=20,
    scoring="average_precision",  # PR-AUC
    cv=3,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# If you used SMOTE: tune on (X_train_sm, y_train_sm)
search.fit(X_train_sm, y_train_sm)

print("Best Params:", search.best_params_)
best_rf = search.best_estimator_

"""Final optimization idea"""

proba = best_rf.predict_proba(X_test)[:, 1]

best_t, best_f1 = None, -1
for t in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:
    pred = (proba >= t).astype(int)
    f1 = f1_score(y_test, pred)
    if f1 > best_f1:
        best_f1, best_t = f1, t

print("Best threshold for F1:", best_t, "Best F1:", best_f1)